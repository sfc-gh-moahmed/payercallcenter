{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ff8eb8-9131-4fa2-9805-62ffc561ce5e",
   "metadata": {
    "collapsed": false,
    "name": "intro_md",
    "resultHeight": 371
   },
   "source": [
    "# Payer Call Center Assistant Unstructured Data Setup\n",
    "\n",
    "In this **Container Runtime** Notebook, we will **prepare all the unstructured data** needed before we can run the Payer Call Center Assistant Streamlit App. Once this data is processed, the chatbot will have a rich knowledge base to start from that's all stored within the [Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) service, a fully managed indexing and retrieval service. Cortex Search will then be used for RAG.\n",
    "\n",
    "There are two types of data we're dealing with in this solution:\n",
    "- **Audio files**: previously recorded calls between a call center agent and a member\n",
    "- **PDF files**: FAQ docs for call center agents to help answer member inquiries\n",
    "\n",
    "**Why is Container Runtime needed?**\\\n",
    "Since we have audio files, we will need to install OpenAI Whisper in order to transcribe those files into text. OpenAI Whisper requires `ffmpeg` to be installed, which cannot be installed in Warehouse compute. We will also use GPU compute here, which makes it much faster to transcribe these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e69a9-8199-4718-ab10-a1da6c2e7007",
   "metadata": {
    "collapsed": false,
    "name": "cortex_search_md",
    "resultHeight": 180
   },
   "source": [
    "### Cortex Search \n",
    "\n",
    "Cortex Search gets you up and running with a hybrid (vector and keyword) search engine on your text data in minutes, without having to worry about embedding, infrastructure maintenance, search quality parameter tuning, or ongoing index refreshes.\n",
    "\n",
    "It powers a broad array of search experiences for Snowflake users including [Retrieval Augmented Generation (RAG)](Retrieval Augmented Generation (RAG)) applications leveraging Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4962c6-ae45-448f-8f82-2b6a7417a2d0",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "cortex_search_img"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.image(\"cortex_search.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd3394-7af9-4c7b-8e47-9442d446cefb",
   "metadata": {
    "collapsed": false,
    "name": "cortex_rag_md",
    "resultHeight": 253
   },
   "source": [
    "### Cortex Search for RAG\n",
    "\n",
    "Retrieval augmented generation (RAG) is a technique for retrieving data from a knowledge base to enhance the generated response of a large language model. The following architecture diagram shows how you can combine Cortex Search with [Cortex LLM Functions](Cortex LLM Functions) to create enterprise chatbots with RAG using your Snowflake data as a knowledge base.\n",
    "\n",
    "#### Using Cortex Search for RAG in Snowflake\n",
    "Cortex Search is the retrieval engine that provides the Large Language Model with the context it needs to return answers that are grounded in your most up-to-date proprietary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef14cf-72a9-4702-a5ae-258ea8276ad1",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "cortex_search_rag_img"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.image(\"cortex_search_rag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f631c1c-a8a6-40b0-9de9-1525e26308c3",
   "metadata": {
    "collapsed": false,
    "name": "start_md",
    "resultHeight": 60
   },
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86573c-5555-42b1-acf2-d31296d7075f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import glob\n",
    "\n",
    "session = get_active_session()\n",
    "root = Root(session)\n",
    "\n",
    "# Add a query tag to the session. This helps with debugging and performance monitoring.\n",
    "session.query_tag = {\"origin\":\"sf_sit\", \n",
    "                     \"name\":\"payer_call_center_assistant\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0},\n",
    "                     \"attributes\":{\"is_quickstart\":0, \"source\":\"notebook\"}}\n",
    "\n",
    "# Set session context \n",
    "session.use_role(f\"CORTEX_CUSTOMER_HCLSPAYERCALLCENTERASSISTANT_DATA_SCIENTIST\") \n",
    "\n",
    "# Print the current role, warehouse, and database/schema\n",
    "print(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebd2bf-f46b-40bd-ba3b-979bb0dca196",
   "metadata": {
    "collapsed": false,
    "name": "part1_transcribe_audio_md",
    "resultHeight": 195
   },
   "source": [
    "## 1. Transcribe Audio Files\n",
    "\n",
    "For this portion, we will download OpenAI's [whisper](https://github.com/openai/whisper) model (a pretrained model), and use it for inference. In this case, we're just passing audio files to the model to output transcriptions. \n",
    "\n",
    "In order to install `whisper`, we'll need `ffmpeg`, and there's a provided shell script within the Notebook files repo to get it installed since it's not a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b348e-42d4-4a2b-b52b-27c887faf7fb",
   "metadata": {
    "collapsed": false,
    "name": "install_ffmpeg_md",
    "resultHeight": 41
   },
   "source": [
    "First, install `ffmpeg` by running the setup script provided in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca3cb1-bcde-42a4-adcd-83e5b2d95c1d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "install_ffmpeg"
   },
   "outputs": [],
   "source": [
    "# Run this script to install ffmpeg\n",
    "!sh ffmpeg_install.sh > out.log 2> err.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88231e-f173-406a-8dec-bfe0aaf901ef",
   "metadata": {
    "language": "python",
    "name": "optional_logs"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to see the installation logs\n",
    "#!cat out.log\n",
    "#!cat err.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596db8d3-4129-4edb-b9f8-9f9c436c8838",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "check_installation"
   },
   "outputs": [],
   "source": [
    "# Make sure it got installed\n",
    "!which ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce92033-144c-48e2-b79d-21498cff55f6",
   "metadata": {
    "collapsed": false,
    "name": "install_whisper_md",
    "resultHeight": 41
   },
   "source": [
    "Now, we install OpenAI's Whisper model to transcribe the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e08ae8-1cd6-481a-bed1-0a690c940c14",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "install_whisper"
   },
   "outputs": [],
   "source": [
    "# Install whisper\n",
    "\n",
    "# Note: --quiet suppresses the output. \n",
    "#       You can remove it if you'd like to \n",
    "#       see all the installation messages.\n",
    "\n",
    "!pip install openai-whisper --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac2fb7-5017-4c5e-a25d-c1fb3fc9e39c",
   "metadata": {
    "collapsed": false,
    "name": "load_model_md"
   },
   "source": [
    "Now, we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda741a-b6d0-4f97-87c7-d43237048411",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_whisper"
   },
   "outputs": [],
   "source": [
    "# Load whisper model\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d22125-488b-457f-b987-11d316766d38",
   "metadata": {
    "collapsed": false,
    "name": "download_files_md"
   },
   "source": [
    "Our audio files live in a stage, so we'll download them into this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96734c-391c-48ff-984c-fe52dc0e4f03",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "download_audio_files"
   },
   "outputs": [],
   "source": [
    "# Download all files from stage\n",
    "f = session.file.get('@RAW_DATA/CALL_RECORDINGS/', 'call_recordings/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd274f-552f-4e79-8803-229434812d53",
   "metadata": {
    "collapsed": false,
    "name": "helper_func_md"
   },
   "source": [
    "We'll create a helper function to transcribe the audio, which includes a few audio processing steps before it's ready to pass to the model to decode the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ee8b9-0a35-437a-a923-11bdcc9266ff",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_transcription_function"
   },
   "outputs": [],
   "source": [
    "# Create function to transcribe all audio\n",
    "def transcribe_audio(audio_file_name):\n",
    "    '''\n",
    "        Transcribe audio files\n",
    "    '''\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    print(f\"Transcribing: {audio_file_name}\")\n",
    "    audio = whisper.load_audio(audio_file_name)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    \n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "    \n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "    \n",
    "    # return the recognized text\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d722b8e-f3f2-4c2b-8204-57d62d393186",
   "metadata": {
    "collapsed": false,
    "name": "apply_function_md"
   },
   "source": [
    "We'll apply this function to all our audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defe869-4e8e-48ca-8287-2452e7c7ce63",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "process_audtio_files"
   },
   "outputs": [],
   "source": [
    "# Process all audio files and store in a list\n",
    "audio_files = glob.glob('call_recordings/*.mp3')\n",
    "\n",
    "all_transcribed = []\n",
    "\n",
    "for f in audio_files:\n",
    "    all_transcribed.append((f, transcribe_audio(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e9d4b-e1ef-4793-87a5-40b68442a4de",
   "metadata": {
    "collapsed": false,
    "name": "check_transcriptions_md"
   },
   "source": [
    "Let's take a look at a few of the transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b1367-2b4b-40d5-a238-ac7dc85806bd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "check_transcriptions"
   },
   "outputs": [],
   "source": [
    "# Look at a few of the transcriptions\n",
    "all_transcribed[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff721a-0653-4fd5-8a7b-2ba5e96a0d43",
   "metadata": {
    "collapsed": false,
    "name": "write_results_md"
   },
   "source": [
    "Now we'll store all the results in a Snowpark DF and write it to a Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b869b4-d675-441a-bded-e65f9b4965e9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_df_for_results"
   },
   "outputs": [],
   "source": [
    "# Create a Snowpark DataFrame from the transcriptions\n",
    "df = session.create_dataframe(all_transcribed, schema=[\"AUDIO_FILE_NAME\", \"TRANSCRIPT\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878bc98-e7b8-47ff-831f-3602ef679c6c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_results_to_table"
   },
   "outputs": [],
   "source": [
    "# Save results as a Snowflake Table\n",
    "df.write.mode(\"overwrite\").save_as_table(\"CALL_RECORDINGS_TRANSCRIPT_TABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f16680-3449-45cf-93ee-e28f874a2f44",
   "metadata": {
    "collapsed": false,
    "name": "audio_cortex_search_md"
   },
   "source": [
    "Finally, we create a Cortex Search service on top of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635b441-f94d-4a2d-a425-137059697bec",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "audio_cortex_search"
   },
   "outputs": [],
   "source": [
    "-- Create Cortex Search Service\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE CALL_CENTER_RECORDING_SEARCH\n",
    "ON CHUNK\n",
    "WAREHOUSE = CORTEX_CUSTOMER_HCLSPAYERCALLCENTERASSISTANT_DS_WH\n",
    "TARGET_LAG = '1 Day'\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        TRANSCRIPT AS CHUNK,\n",
    "        AUDIO_FILE_NAME AS RELATIVE_PATH\n",
    "    FROM\n",
    "        CALL_RECORDINGS_TRANSCRIPT_TABLE\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c0ff8-0d15-4798-8558-6ea05f863302",
   "metadata": {
    "collapsed": false,
    "name": "test_audio_cortex_md"
   },
   "source": [
    "We can quickly test the service to make sure it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e9f88-32a7-4897-a2f7-08571377bccc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_audio_cortex_search"
   },
   "outputs": [],
   "source": [
    "# Test out the service\n",
    "\n",
    "response = (root.databases[session.get_current_database()]\n",
    "                 .schemas[session.get_current_schema()]\n",
    "                 .cortex_search_services[\"CALL_CENTER_RECORDING_SEARCH\"]\n",
    "                 .search(\n",
    "                            'Can you give me a summary from the previous call made by Jim Pacheco',\n",
    "                              ['CHUNK',\n",
    "                               'RELATIVE_PATH'],\n",
    "                         limit=3\n",
    "                         )\n",
    "    )\n",
    "\n",
    "results = response.results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba9fa6-5f78-422c-9406-6c18b995c0f8",
   "metadata": {
    "collapsed": false,
    "name": "part2_process_pdfs_md"
   },
   "source": [
    "## 2. Process PDF Files\n",
    "\n",
    "For this portion, we'll create a Python UDF using the SQL API to read and chunk a PDF using open source libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0ad68-10df-4f66-9a16-e22e5d3f89ae",
   "metadata": {
    "collapsed": false,
    "name": "pdf_chunker_udf_md"
   },
   "source": [
    "We first create a Python UDF to read and chunk PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2966e-087b-4a71-9df9-6a234d7a839d",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "pdf_chunker_udf"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION PDF_TEXT_CHUNKER(\"FILE_URL\" VARCHAR(16777216))\n",
    "RETURNS TABLE (\"CHUNK\" VARCHAR(16777216))\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.9'\n",
    "PACKAGES = ('snowflake-snowpark-python','PyPDF2','langchain')\n",
    "HANDLER = 'pdf_text_chunker'\n",
    "AS '\n",
    "from snowflake.snowpark.types import StringType,StructField,StructType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "import PyPDF2, io\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "class pdf_text_chunker:\n",
    "    \n",
    "    def read_pdf(self,file_url:str) ->str:\n",
    "\n",
    "        logger = logging.getLogger(\"udf_logger\")\n",
    "        logger.info(f\"Opening the file path {file_url}\")\n",
    "\n",
    "        with SnowflakeFile.open(file_url,mode=\"rb\") as f:\n",
    "            buffer = io.BytesIO(f.read())\n",
    "        \n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text += page.extract_text().replace(\"\\\\n\",\" \").replace(\"\\\\0\",\" \")\n",
    "            except:\n",
    "                text = \"Unable to extract\"\n",
    "                logger.warn(f\"Unable to extract text from the pdf file {file_url}, page {page}\")\n",
    "        return text\n",
    "\n",
    "    def process(self,file_url:str):\n",
    "        text = self.read_pdf(file_url)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4000,\n",
    "            chunk_overlap=400,\n",
    "            length_function = len\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        df = pd.DataFrame(chunks,columns=[\"chunks\"])\n",
    "\n",
    "        yield from df.itertuples(index=False, name=None)\n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266b497-91df-4bcb-bd61-a64323a5c2d9",
   "metadata": {
    "collapsed": false,
    "name": "process_pdfs_md"
   },
   "source": [
    "Then, apply the UDF to the PDFs stored in our stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace41417-a3ff-4692-97bd-ca90ab6ebaf8",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "process_pdfs"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE FAQ_DOCS_CHUNKS_TABLE ( \n",
    "    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n",
    "    CHUNK VARCHAR(16777216) -- Piece of text\n",
    ");\n",
    "\n",
    "INSERT INTO FAQ_DOCS_CHUNKS_TABLE (RELATIVE_PATH, CHUNK)\n",
    "SELECT RELATIVE_PATH, func.CHUNK AS CHUNK\n",
    "FROM (\n",
    "    SELECT *\n",
    "    FROM DIRECTORY(@RAW_DATA)\n",
    "    WHERE RELATIVE_PATH LIKE '%FAQ%'\n",
    "),\n",
    "TABLE(PDF_TEXT_CHUNKER(build_scoped_file_url(\n",
    "    @RAW_DATA,\n",
    "    relative_path))) AS func;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe395dc-5e8b-4d01-b6ac-56a3040ff09f",
   "metadata": {
    "collapsed": false,
    "name": "check_chunks_md"
   },
   "source": [
    "Let's make sure the files were properly read and chunked now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3309947-6af7-4645-8c46-00ec2c1b618d",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "check_chunks"
   },
   "outputs": [],
   "source": [
    "-- Make sure files were properly read and chunked\n",
    "SELECT * FROM FAQ_DOCS_CHUNKS_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f21d-3885-41b4-acf6-3cd2b654a3fa",
   "metadata": {
    "collapsed": false,
    "name": "pdf_cortex_search_md"
   },
   "source": [
    "Finally, we create a Cortex Search service on top of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ce5bc-ca83-4612-85f4-62c388f75c49",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "pdf_cortex_search"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE CALL_CENTER_FAQ_SEARCH\n",
    "ON CHUNK\n",
    "WAREHOUSE = CORTEX_CUSTOMER_HCLSPAYERCALLCENTERASSISTANT_DS_WH\n",
    "TARGET_LAG = '1 Day'\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        CHUNK,\n",
    "        RELATIVE_PATH\n",
    "    FROM\n",
    "        FAQ_DOCS_CHUNKS_TABLE\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedecbd-2d51-4c76-9b3b-16d94360e1f2",
   "metadata": {
    "collapsed": false,
    "name": "test_pdf_cortex_md"
   },
   "source": [
    "We can quickly test the service to make sure it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1109067-e9c8-4768-933b-07f2dcb54b46",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_pdf_cortex_search"
   },
   "outputs": [],
   "source": [
    "# Test out the service\n",
    "\n",
    "response = (root.databases[session.get_current_database()]\n",
    "                 .schemas[session.get_current_schema()]\n",
    "                 .cortex_search_services[\"CALL_CENTER_FAQ_SEARCH\"]\n",
    "                 .search(\n",
    "                     'Were there any revisions to COVID related coverages?',\n",
    "                     ['CHUNK','RELATIVE_PATH'], limit=3\n",
    "                        )\n",
    "           )\n",
    "\n",
    "results = response.results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66c8d8-545b-4cd6-98fc-0114c462467d",
   "metadata": {
    "collapsed": false,
    "name": "conclusion"
   },
   "source": [
    "### :tada: All the unstructured data is now processed and ready to be used by the Payer Call Center Assistant Streamlit App!"
   ]
  }
 ]
}